{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe61f31a",
   "metadata": {},
   "source": [
    "Generate a spectrogram from a .wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5603b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20.564884  , -29.787586  ,  -7.3620214 , ...,   0.725213  ,\n",
       "         -0.99411374,  -3.0126836 ],\n",
       "       [-19.39139   , -14.051232  ,  -8.910709  , ...,  -2.9069686 ,\n",
       "         -9.905552  ,  -3.6721606 ],\n",
       "       [-18.209446  , -12.80246   , -25.530844  , ..., -12.095527  ,\n",
       "         -5.5859337 ,  -4.383841  ],\n",
       "       ...,\n",
       "       [-47.91812   , -44.987164  , -48.860184  , ..., -49.969803  ,\n",
       "        -49.969803  , -47.85353   ],\n",
       "       [-49.969803  , -49.969803  , -47.564568  , ..., -49.969803  ,\n",
       "        -49.969803  , -46.341     ],\n",
       "       [-49.969803  , -49.969803  , -49.870384  , ..., -49.396095  ,\n",
       "        -44.577984  , -44.82728   ]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import IPython.display as ipd \n",
    "import librosa\n",
    "# import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def spectrogram_from_file(file_path, max_freq=10000):\n",
    "    samples, sampling_rate = librosa.load(file_path, sr=None, mono=True, offset=0.0, duration=None)\n",
    "    X = librosa.stft(samples)\n",
    "    return librosa.amplitude_to_db(abs(X))\n",
    "    \n",
    "#     plt.figure(figsize=(14, 5))\n",
    "#     librosa.display.specshow(Xdb, sr=sampling_rate, x_axis='time', y_axis='hz')\n",
    "#     plt.colorbar()\n",
    "\n",
    "spectrogram_from_file(\"audio/hello.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e8e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "hello_value = 0\n",
    "goodbye_value = 1\n",
    "\n",
    "for i in range(1, 21):\n",
    "    X.append(spectrogram_from_file(f\"audio/hello/{i}.wav\"))\n",
    "    y.append(hello_value)\n",
    "\n",
    "for i in range(1, 21):\n",
    "    X.append(spectrogram_from_file(f\"audio/goodbye/{i}.wav\"))\n",
    "    y.append(goodbye_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6a2234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 196, 1025)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "maximum_X = max([i.shape[1] for i in X])\n",
    "# Pad the values of X with 0s upto the maximum time steps and transpose the matrix\n",
    "X = [np.pad(i, [(0,0), (0, maximum_X - i.shape[1])], constant_values=(0,)).T for i in X]\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69801979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 11:00:20.903291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-03 11:00:20.903432: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-03-03 11:00:23.034635: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-03 11:00:23.034779: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-03 11:00:23.034798: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ollie-ThinkPad-L490): /proc/driver/nvidia/version does not exist\n",
      "2022-03-03 11:00:23.036101: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 11:00:23.294430: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 24108000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 102ms/step - loss: 0.2496 - accuracy: 0.4333\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2466 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2527 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.2510 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.2485 - accuracy: 0.5333\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.2494 - accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.2477 - accuracy: 0.5333\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.2474 - accuracy: 0.5333\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2469 - accuracy: 0.5667\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.2455 - accuracy: 0.5333\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.2458 - accuracy: 0.5333\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2443 - accuracy: 0.5333\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.2451 - accuracy: 0.5333\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.2451 - accuracy: 0.5333\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2441 - accuracy: 0.5333\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.2431 - accuracy: 0.5333\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.2439 - accuracy: 0.5333\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2419 - accuracy: 0.5333\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.2416 - accuracy: 0.5333\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.2412 - accuracy: 0.5333\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2390 - accuracy: 0.5333\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2385 - accuracy: 0.5333\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.2387 - accuracy: 0.5333\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.2386 - accuracy: 0.5333\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.2385 - accuracy: 0.5333\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2354 - accuracy: 0.5333\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.2365 - accuracy: 0.5667\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.2367 - accuracy: 0.5667\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.2346 - accuracy: 0.5667\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.2335 - accuracy: 0.5667\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.2319 - accuracy: 0.5667\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2320 - accuracy: 0.5667\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.2302 - accuracy: 0.6000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2309 - accuracy: 0.6000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2315 - accuracy: 0.6000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2321 - accuracy: 0.5667\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2314 - accuracy: 0.5667\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.2303 - accuracy: 0.5667\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2290 - accuracy: 0.5667\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.2325 - accuracy: 0.5667\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.2318 - accuracy: 0.5667\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.2323 - accuracy: 0.5667\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2286 - accuracy: 0.6000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.2291 - accuracy: 0.5667\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2284 - accuracy: 0.5333\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.2311 - accuracy: 0.5667\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.2256 - accuracy: 0.5667\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.2281 - accuracy: 0.5333\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2295 - accuracy: 0.5333\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.2318 - accuracy: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a344e2ca0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units = 50, return_sequences = False, input_shape = (196, 1025)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7353980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50)                215200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 215,251\n",
      "Trainable params: 215,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# regressor.predict(X_train)\n",
    "# regressor(X_test).shape\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a366a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02591529],\n",
       "       [0.56701875],\n",
       "       [0.4258993 ],\n",
       "       [0.7155206 ],\n",
       "       [0.69749135],\n",
       "       [0.54094255],\n",
       "       [0.5988152 ],\n",
       "       [0.54193765],\n",
       "       [0.47189283],\n",
       "       [0.55193925]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425856c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e12d0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 341ms/step - loss: 0.2318 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23179180920124054, 0.5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c537913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ollie/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio/hello/hello.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/librosa/core/audio.py:155\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/soundfile.py:629\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/soundfile.py:1183\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m-> 1183\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_snd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msf_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_ptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{0!r}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/soundfile.py:1357\u001b[0m, in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1356\u001b[0m err_str \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error_number(err)\n\u001b[0;32m-> 1357\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(prefix \u001b[38;5;241m+\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(err_str)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening 'audio/hello/hello.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m([spectrogram\u001b[38;5;241m.\u001b[39mshape])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mreshape(spectrogram, [\u001b[38;5;241m1\u001b[39m, spectrogram\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], spectrogram\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio/hello/hello.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mevaluate_file\u001b[0;34m(model, file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_file\u001b[39m(model, file_path):\n\u001b[0;32m----> 2\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mspectrogram_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m([spectrogram\u001b[38;5;241m.\u001b[39mshape])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mreshape(spectrogram, [\u001b[38;5;241m1\u001b[39m, spectrogram\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], spectrogram\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mspectrogram_from_file\u001b[0;34m(file_path, max_freq)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspectrogram_from_file\u001b[39m(file_path, max_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     samples, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mstft(samples)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m librosa\u001b[38;5;241m.\u001b[39mamplitude_to_db(\u001b[38;5;28mabs\u001b[39m(X))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/librosa/core/audio.py:174\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    173\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (exc)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/librosa/core/audio.py:198\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"Load an audio buffer using audioread.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mThis loads one block at a time, and then concatenates the results.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    199\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m    200\u001b[0m     n_channels \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39mchannels\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/audioread/__init__.py:111\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/voice-recognition-TGg8XC8-/lib/python3.8/site-packages/audioread/rawread.py:62\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio/hello/hello.wav'"
     ]
    }
   ],
   "source": [
    "def evaluate_file(model, file_path):\n",
    "    spectrogram = np.array(spectrogram_from_file(file_path)).T\n",
    "    print([spectrogram.shape])\n",
    "    return model.predict(np.reshape(spectrogram, [1, spectrogram.shape[0], spectrogram.shape[1]]))[0][0]\n",
    "\n",
    "evaluate_file(model, \"audio/hello/hello.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice-recognition",
   "language": "python",
   "name": "voice-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
